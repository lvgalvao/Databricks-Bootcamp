# Segunda aula

Nosso objetivo nessa primeira aula √© configurar nosso ambiente no Databricks, ter uma vis√£o da arquitetura geral do Spark (e do Databricks) e vamos ter um hands-on de c√≥digo.

## Configura√ß√£o Databricks e Github

## Ingest√£o da API

Ingerir dados da API da **Coinbase** que fornece o pre√ßo atual do Bitcoin. Esses dados devem ser salvos em uma **Delta Table** no Databricks a cada 15 minutos, garantindo consist√™ncia, escalabilidade e consulta eficiente no Delta Lake.

---

### **C√≥digo**

1. **Consumo da API:**
   - O c√≥digo acessa a API da Coinbase (`https://api.coinbase.com/v2/prices/spot?currency=USD`) para obter o pre√ßo atual do Bitcoin.
   - Ele extrai os dados no formato JSON e converte para um formato estruturado no PySpark DataFrame.

```python
import requests
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, FloatType

# Inicializar Spark
spark = SparkSession.builder.appName("Bitcoin Price Integration").getOrCreate()

# Configura√ß√µes
API_URL = "https://api.coinbase.com/v2/prices/spot?currency=USD"
DELTA_PATH = "/mnt/delta/bitcoin_price"

def fetch_bitcoin_price():
    """Obt√©m o pre√ßo atual do Bitcoin da API Coinbase."""
    try:
        response = requests.get(API_URL)
        response.raise_for_status()
        data = response.json()['data']
        return {
            "amount": float(data['amount']),
            "base": data['base'],
            "currency": data['currency']
        }
    except requests.exceptions.RequestException as e:
        raise Exception(f"Erro ao acessar a API: {e}")

def save_to_delta(data):
    """Salva os dados no Delta Lake."""
    # Schema para DataFrame
    schema = StructType([
        StructField("amount", FloatType(), True),
        StructField("base", StringType(), True),
        StructField("currency", StringType(), True)
    ])
    
    # Converter para DataFrame
    data_df = spark.createDataFrame([data], schema=schema)
    
    # Salvar no Delta Lake
    data_df.write.format("delta").mode("append").save(DELTA_PATH)
    print(f"Dados salvos em Delta Table no caminho: {DELTA_PATH}")

if __name__ == "__main__":
    print("Iniciando integra√ß√£o com API...")
    bitcoin_price = fetch_bitcoin_price()
    print(f"Pre√ßo do Bitcoin: {bitcoin_price['amount']} {bitcoin_price['currency']}")
    save_to_delta(bitcoin_price)
    print("Pipeline conclu√≠do com sucesso.")
```

## 2. **Escrita no Delta Lake:**
   - O dado extra√≠do √© salvo no **Delta Lake** no caminho `/mnt/delta/bitcoin_price`.
   - **Delta Lake** √© usado para:
     - Garantir consist√™ncia (ACID transactions).
     - Registrar mudan√ßas com logs transacionais (`_delta_log`).
     - Permitir consultas eficientes e escal√°veis.

**O Caminho `/mnt/` no Contexto do Databricks**

No Databricks, o diret√≥rio `/mnt/` √© um **ponto de montagem (mount point)** que conecta o cluster do Databricks a um sistema de armazenamento externo, como:

- **AWS S3**
- **Azure Data Lake Storage (ADLS)**
- **Google Cloud Storage (GCS)**

Esse ponto de montagem permite que o Databricks acesse e gerencie os dados como se estivessem no sistema de arquivos local, enquanto na realidade est√£o em um servi√ßo de armazenamento remoto.

---

**Como `/mnt/` Funciona**

1. **Configura√ß√£o Inicial:**
   - O diret√≥rio `/mnt/` √© configurado pelo administrador do Databricks ou pelo usu√°rio com permiss√µes apropriadas.
   - Ele utiliza comandos de montagem (`dbutils.fs.mount`) para conectar a um bucket S3, uma conta ADLS ou outro armazenamento.

2. **Mapeamento para o Armazenamento Externo:**
   - Cada subdiret√≥rio no `/mnt/` representa um caminho no sistema de armazenamento remoto.
   - Por exemplo:
     - `/mnt/delta/bitcoin_price` ‚Üí `s3://my-databricks-bucket/mnt/delta/bitcoin_price`

3. **Uso no Delta Lake:**
   - Quando voc√™ salva dados no caminho `/mnt/delta/bitcoin_price`:
     - Os arquivos s√£o fisicamente gravados no bucket correspondente no S3, mas podem ser acessados no Databricks como se fossem locais.
   - O Delta Lake organiza os arquivos em:
     - **Arquivos de Dados:** Salvos no formato **Parquet**.
     - **Delta Logs (`_delta_log`):** Gerenciam transa√ß√µes ACID e versionamento.

---

**O Caminho `/mnt/delta/bitcoin_price`**
No contexto do seu pipeline:

1. **Localiza√ß√£o F√≠sica:**
   - Este caminho est√° mapeado para o sistema de armazenamento remoto (ex.: AWS S3).
   - Exemplo: `s3://my-databricks-bucket/delta/bitcoin_price/`

2. **Estrutura dos Dados:**
   - Diret√≥rio com:
     - **Arquivos Parquet:** Cont√™m os dados salvos.
     - **Diret√≥rio `_delta_log`:** Armazena logs transacionais, permitindo hist√≥rico e consist√™ncia.

3. **Acessibilidade:**
   - No Databricks, os dados podem ser acessados diretamente pelo caminho `/mnt/delta/bitcoin_price` em APIs PySpark:
     ```python
     df = spark.read.format("delta").load("/mnt/delta/bitcoin_price")
     df.show()
     ```

**Resumo**
O diret√≥rio `/mnt/` √© o ponto de integra√ß√£o entre o Databricks e sistemas de armazenamento externo, como AWS S3. No seu caso:
- **`/mnt/delta/bitcoin_price`** √© um subdiret√≥rio montado para armazenar os dados do pipeline no Delta Lake.
- Ele organiza os dados em formato Delta, garantindo efici√™ncia, consist√™ncia e suporte a transa√ß√µes ACID.
- √â uma pr√°tica recomendada para pipelines escal√°veis e integrados com sistemas de armazenamento na nuvem.

---

3. **Rotina de Execu√ß√£o:**
   - Esse c√≥digo √© projetado para ser executado a cada 15 minutos, de forma automatizada, via **Databricks Workflow**.

---

### **Como Colocar o C√≥digo em um Workflow no Databricks**

#### **1. Subir o C√≥digo no Databricks**
- Salve o c√≥digo como um **notebook** no Databricks. Exemplo: `bitcoin_price_ingestion`.

#### **2. Criar o Workflow**
1. Acesse a aba **Workflows** no Databricks.
2. Clique em **Create Job**.
3. Configure o Workflow:
   - **Name:** Nome do job, por exemplo: `Bitcoin Price Ingestion`.
   - **Task:** Nome da task, por exemplo:  `bitcoin_price_ingestion`
   - **Notebook:** Selecione o notebook `src/bitcoin_price_ingestion` que cont√©m o c√≥digo.
   - **Cluster:** Escolha ou configure um cluster para executar o job.

A diferen√ßa entre um **cluster Serverless** e um **cluster normal** no Databricks est√° na forma como os recursos s√£o gerenciados, provisionados e otimizados. Aqui est√° uma explica√ß√£o detalhada no `README-cluster.md`

   - **Schedule:** Defina para executar a cada 15 minutos:
     - **Cron Expression:** `0 */15 * ? * * *` (executa a cada 15 minutos).

4. **Adicionar Regras de Falha:**
   - Configure o Workflow para:
     - Reexecutar em caso de falha (ex.: 3 tentativas, com intervalo de 5 minutos).

---

### **Rotina para Inserir Dados a Cada 15 Minutos**

Sim, √© necess√°rio garantir que cada execu√ß√£o insira os dados de forma incremental na **Delta Table**. Isso √© feito automaticamente ao usar o modo `append` no c√≥digo de escrita:

```python
data_df.write.format("delta").mode("append").save("/mnt/delta/bitcoin_price")
```

#### **Como Funciona a Rotina Incremental?**
1. **Cada Execu√ß√£o (15 Minutos):**
   - Um novo registro √© consumido da API.
   - O dado √© salvo como uma nova linha na tabela Delta.

2. **Delta Lake Logs (`_delta_log`):**
   - Cada inser√ß√£o √© registrada no log transacional, garantindo que n√£o haja conflitos ou inconsist√™ncias.

---

### **Como Jogar os Dados em uma Tabela Gerenciada pelo Databricks**

Agora que os dados est√£o armazenados no **Delta Lake** no caminho `/mnt/delta/bitcoin_price/`, voc√™ pode registrar esse local como uma **tabela gerenciada pelo Databricks** usando SQL ou PySpark.

---

#### **1. Criar uma Tabela Gerenciada no Databricks**
Para registrar a tabela no cat√°logo, use o seguinte comando SQL no Databricks:

```sql
CREATE TABLE bitcoin_price
USING DELTA
LOCATION '/mnt/delta/bitcoin_price';
```

#### **2. Explica√ß√£o do Comando**
- **`CREATE TABLE`**: Cria a tabela no cat√°logo do Databricks.
- **`USING DELTA`**: Especifica que os dados no caminho s√£o armazenados no formato Delta.
- **`LOCATION`**: Aponta para o caminho dos dados no Delta Lake.

#### **3. Resultado**
- Os metadados da tabela s√£o registrados no cat√°logo (Unity Catalog ou Hive Metastore, dependendo da configura√ß√£o).
- Agora voc√™ pode consultar a tabela diretamente no Databricks:
  ```sql
  SELECT * FROM bitcoin_price;
  ```

---

### **Resumo de Todo o Processo**

Aqui est√° o resumo detalhado de todo o processo, desde o consumo da API at√© o armazenamento no Databricks e na AWS:

---

#### **1. Escrita da API**
- Voc√™ consumiu os dados da **API Coinbase** para obter o pre√ßo do Bitcoin.
- O dado foi estruturado no formato JSON e convertido para um **DataFrame Spark** com schema definido.

---

#### **2. Armazenamento no Databricks**
- O comando de escrita no Delta Lake:
  ```python
  data_df.write.format("delta").mode("append").save("/mnt/delta/bitcoin_price")
  ```
  salvou os dados no caminho **`/mnt/delta/bitcoin_price/`**.

- **O que acontece aqui?**
  - Os dados foram gravados como arquivos **`Parquet`**.
  - Um diret√≥rio `_delta_log` foi criado para gerenciar as transa√ß√µes (log de mudan√ßas no Delta Lake).

---

#### **3. Armazenamento na AWS**
- O caminho **`/mnt/delta/bitcoin_price/`** est√° montado em um **bucket S3**, como parte da integra√ß√£o do Databricks com o armazenamento AWS.
- Os dados ficam fisicamente armazenados no bucket S3 configurado para o ambiente Databricks, por exemplo:
  ```
  s3://my-databricks-bucket/delta/bitcoin_price/
  ```
- **O que fica no S3?**
  - Arquivos de dados (`Parquet`) contendo o conte√∫do salvo.
  - Diret√≥rio `_delta_log`, que registra as opera√ß√µes no Delta Lake para versionamento e transa√ß√µes ACID.

---

#### **4. Registro no Cat√°logo**
- **Unity Catalog ou Hive Metastore:**
  - O cat√°logo armazena os **metadados da tabela**.
  - Quando voc√™ usa `CREATE TABLE ... LOCATION`, ele registra o schema da tabela e o caminho no Delta Lake.
- **Benef√≠cio:**
  - Permite consultar os dados sem precisar especificar o caminho f√≠sico.
  - Exemplo de consulta:
    ```sql
    SELECT * FROM bitcoin_price;
    ```

---

#### **5. Consulta e Uso**
- Ap√≥s o registro, voc√™ pode:
  - Consultar os dados no Databricks com SQL ou PySpark.
  - Utilizar a tabela em jobs e workflows para an√°lises automatizadas.
  - Monitorar o acesso e as permiss√µes via Unity Catalog (se configurado).

---

### **Fluxo Completo**
1. **API Coinbase:** Consome os dados.
2. **Delta Lake no Databricks:**
   - Dados armazenados no caminho `/mnt/delta/bitcoin_price/`.
   - Fisicamente localizado no S3.
3. **Cat√°logo:** Registra os metadados da tabela com `CREATE TABLE`.
4. **Consultas:** Permite acessar dados diretamente pelo nome da tabela no Databricks.

---

Se precisar de ajuda para configurar ou entender algum passo espec√≠fico, √© s√≥ avisar! üòä