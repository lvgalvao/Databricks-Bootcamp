{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef68047-871f-4219-8666-3894c67b241e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Configurações de conexão com o PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://dpg-cuciu352ng1s73b52bgg-a.ohio-postgres.render.com:5432/transactions_yz0m\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"transactions_yz0m_user\",\n",
    "    \"password\": \"66BsNmXaNh3btK1ZY8GLbnrFmwUFK9xY\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Nome da tabela no PostgreSQL\n",
    "postgres_table = \"transactions\"\n",
    "\n",
    "# Carregar apenas os dados incrementais do PostgreSQL\n",
    "@dlt.view(\n",
    "    comment=\"Carrega os dados incrementais de transações do PostgreSQL\"\n",
    ")\n",
    "def raw_transactions_incremental():\n",
    "    # Obter o último `transaction_date` da tabela existente\n",
    "    last_transaction_date = (\n",
    "        spark.sql(\"SELECT MAX(transaction_date) AS last_transaction_date FROM raw_transactions\")\n",
    "        .collect()[0][\"last_transaction_date\"]\n",
    "    )\n",
    "\n",
    "    # Criar consulta incremental\n",
    "    if last_transaction_date:\n",
    "        print(f\"Última transação registrada: {last_transaction_date}\")\n",
    "        query = f\"\"\"\n",
    "            (SELECT * FROM {postgres_table}\n",
    "             WHERE transaction_date > '{last_transaction_date}') AS t\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"Nenhum dado encontrado na tabela. Carregando todos os dados.\")\n",
    "        query = f\"(SELECT * FROM {postgres_table}) AS t\"\n",
    "\n",
    "    # Carregar dados do PostgreSQL\n",
    "    return (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", query)\n",
    "        .option(\"user\", jdbc_properties[\"user\"])\n",
    "        .option(\"password\", jdbc_properties[\"password\"])\n",
    "        .option(\"driver\", jdbc_properties[\"driver\"])\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "# Atualizar ou inserir dados na tabela existente\n",
    "@dlt.table(\n",
    "    name=\"raw_transactions\",\n",
    "    comment=\"Tabela Delta gerenciada com dados incrementais de transações\"\n",
    ")\n",
    "def raw_transactions():\n",
    "    # Adicionar os dados incrementais à tabela existente\n",
    "    return dlt.read(\"raw_transactions_incremental\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transactions_ingestion_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
